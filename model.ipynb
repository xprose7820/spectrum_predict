{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import clear_output\n",
    "from typing import Dict\n",
    "from torch.utils.tensorboard.summary import hparams\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=\"same\", stride=1, r=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv3 = nn.Conv1d(in_channels, out_channels, kernel_size=1, padding=padding, stride=stride) if r == True else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv(x)\n",
    "        x1 = self.bn(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.conv2(x1)\n",
    "        x1 = self.bn2(x1)\n",
    "        x2 = self.conv3(x)\n",
    "        return x1+x2\n",
    "    \n",
    "class BottleNeck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose1d(in_channels, out_channels, kernel_size=kernel_size, padding=1, stride=stride)\n",
    "        self.relu = nn.ReLU()\n",
    "       \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels ,skip_channels, scale=1, kernel_size=2, padding=1, stride=2,last=True):\n",
    "        super().__init__()\n",
    "        self.pixel_shuffle = nn.PixelShuffle(scale) if scale > 1 else nn.Identity()\n",
    "        self.conv = nn.ConvTranspose1d(skip_channels, out_channels, kernel_size=kernel_size, padding=0, stride=stride)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()  if last == True else nn.Identity()\n",
    "\n",
    "    def forward(self, x,skip):\n",
    "       \n",
    "            # Ensure x is correctly sized to match skip\n",
    "    \n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        # print(\"Pre-BN size:\", x.shape)  # This should show the number of channels\n",
    "        # print(f\"Channel size before ConvTranspose1d: {x.shape[1]}\")  # Should be 1536\n",
    "\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "\n",
    "        x = self.relu(x)\n",
    "        x = self.relu2(x)\n",
    "      \n",
    "        return x\n",
    "\n",
    "\n",
    "class Runet(nn.Module):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Runet, self).__init__(**kwargs)\n",
    "\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=7, padding=\"same\", stride=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.MaxPool1d(2),\n",
    "            DownBlock(64, 64, r=False),\n",
    "            DownBlock(64, 64, r=False),\n",
    "            DownBlock(64, 64, r=False),\n",
    "            DownBlock(64, 128, r=True))\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.MaxPool1d(2, stride=2),\n",
    "            DownBlock(128, 128, r=False),\n",
    "            DownBlock(128, 128, r=False),\n",
    "            DownBlock(128, 128, r=False),\n",
    "            DownBlock(128, 256, r=True))\n",
    "\n",
    "        self.down4 = nn.Sequential(\n",
    "            nn.MaxPool1d(2),\n",
    "            DownBlock(256, 256, r=False),\n",
    "            DownBlock(256, 256, r=False),\n",
    "            DownBlock(256, 256, r=False),\n",
    "            DownBlock(256, 256, r=False),\n",
    "            DownBlock(256, 256, r=False),\n",
    "            DownBlock(256, 512, r=True))\n",
    "\n",
    "        self.down5 = nn.Sequential(\n",
    "            nn.MaxPool1d(2),\n",
    "            DownBlock(512, 512, r=False),\n",
    "            DownBlock(512, 512, r=False),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.BottleNeck = nn.Sequential(\n",
    "            BottleNeck(512, 1024),\n",
    "            BottleNeck(1024, 512))\n",
    "\n",
    "        self.up1 = UpBlock(512, 512, 1024, scale=1)\n",
    "        self.up2 = UpBlock(512, 384, 1024, scale=1)\n",
    "        self.up3 = UpBlock(384, 256, 640, scale=1)\n",
    "        self.up4 = UpBlock(256, 96, 384, scale=1)\n",
    "\n",
    "        self.up5 = UpBlock(96, 99, 160, scale=1, last=False)\n",
    "        self.conv = nn.Conv1d(99, 1, kernel_size=1, padding=0, stride=1)\n",
    "        self.avg_pool = nn.AvgPool1d(2, stride=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        x5 = self.down5(x4)\n",
    "\n",
    "        x6 = self.BottleNeck(x5)\n",
    "\n",
    "        x7 = self.up1(x6, x5)\n",
    "        # print(\"x4:\", x4.shape)\n",
    "\n",
    "        # print(\"x7:\", x7.shape)\n",
    "\n",
    "        x8 = self.up2(x7, x4)\n",
    "        x9 = self.up3(x8, x3)\n",
    "        x10 = self.up4(x9, x2)\n",
    "        x11 = self.up5(x10, x1)\n",
    "        result = self.conv(x11)\n",
    "        output = self.avg_pool(result)\n",
    "        return output\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
